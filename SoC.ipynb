{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiyushiAnand/Breakout_Genius/blob/main/SoC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxwApZWvoKlX",
        "outputId": "af98973a-dca7-40ca-ffed-0bfd04db1e69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.0.8)\n",
            "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.7.5)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[atari]) (6.0.0)\n",
            "Requirement already satisfied: gym[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (0.0.8)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (8.1.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (4.65.0)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym[atari]\n",
        "!pip install gym[accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "C_YzjPhLHfsR"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import convolve, gaussian\n",
        "\n",
        "import os\n",
        "import io\n",
        "import base64\n",
        "import time\n",
        "import glob\n",
        "from IPython.display import HTML\n",
        "import torch.nn.functional as F\n",
        "from gym.wrappers import AtariPreprocessing\n",
        "from gym.wrappers import FrameStack\n",
        "from gym.wrappers import TransformReward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qDB7aUFKHoe9"
      },
      "outputs": [],
      "source": [
        "def make_env(env_name, clip_rewards = True, seed = None):\n",
        "\t# complete this function which returns an object 'env' using gym module\n",
        "\t# Use AtariPreprocessing, FrameStack, TransformReward(based on the clip_rewards variable passed in the arguments of the function), check their usage from internet\n",
        "\t# Use FrameStack to stack 4 frames\n",
        "\t# TODO\n",
        "  env = gym.make(env_name)\n",
        "  env = AtariPreprocessing(env)\n",
        "  env = FrameStack(env, num_stack=4)\n",
        "  if clip_rewards:\n",
        "        env = TransformReward(env, lambda r: np.sign(r))\n",
        "  if seed is not None:\n",
        "        env.seed(seed)\n",
        "  return env\n",
        "\n",
        "# Initialize the device based on CUDA availability\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "37eZgCBCHwPN"
      },
      "outputs": [],
      "source": [
        "# Next we create a class DQNAgent which is the class containing the neural network, This class is derived from nn.Module\n",
        "\n",
        "class DQNAgent(nn.Module):\n",
        "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
        "\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.n_actions = n_actions\n",
        "        self.state_shape = state_shape\n",
        "\n",
        "        state_dim = state_shape[0]\n",
        "        # a simple NN with state_dim as input vector (inout is state s)\n",
        "        # and self.n_actions as output vector of logits of q(s, a)\n",
        "        self.network = nn.Sequential()\n",
        "        self.network.add_module('conv1', nn.Conv2d(4,16,kernel_size=8, stride=4))\n",
        "        self.network.add_module('relu1', nn.ReLU())\n",
        "        self.network.add_module('conv2', nn.Conv2d(16,32,kernel_size=4, stride=2))\n",
        "        self.network.add_module('relu2', nn.ReLU())\n",
        "        self.network.add_module('flatten', nn.Flatten())\n",
        "        self.network.add_module('linear3', nn.Linear(2592, 256)) #2592 calculated above\n",
        "        self.network.add_module('relu3', nn.ReLU())\n",
        "        self.network.add_module('linear4', nn.Linear(256, n_actions))\n",
        "\n",
        "        self.parameters = self.network.parameters\n",
        "\n",
        "    def forward(self, state_t):\n",
        "        # pass the state at time t through the newrok to get Q(s,a)\n",
        "        qvalues = self.network(state_t)\n",
        "        return qvalues\n",
        "\n",
        "    def get_qvalues(self, states):\n",
        "        # input is an array of states in numpy and outout is Qvals as numpy array\n",
        "        states = torch.tensor(states, device=device, dtype=torch.float32)\n",
        "        qvalues = self.forward(states)\n",
        "        return qvalues.data.cpu().numpy()\n",
        "\n",
        "    def sample_actions(self, qvalues):\n",
        "    # sample actions from a batch of q_values using epsilon greedy policy\n",
        "       epsilon = self.epsilon\n",
        "       if qvalues.ndim == 1:\n",
        "          qvalues = qvalues.reshape(1, -1)  # Reshape to make it a 2D array with batch size 1\n",
        "\n",
        "       batch_size, n_actions = qvalues.shape\n",
        "       random_actions = np.random.choice(n_actions, size=batch_size)\n",
        "       best_actions = qvalues.argmax(axis=-1)\n",
        "       should_explore = np.random.choice([0, 1], batch_size, p=[1 - epsilon, epsilon])\n",
        "       return np.where(should_explore, random_actions, best_actions)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
        "\t# used for evaluationing the trained agent for number of games = n_games and step in each game = t_max\n",
        "\t# returns the mean of sum of all rewards across n_games\n",
        "\t#TODO\n",
        "    total_rewards = 0\n",
        "    for i in range(n_games):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        t = 0\n",
        "\n",
        "        while not done and t < t_max:\n",
        "            q_values = agent.get_qvalues(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
        "\n",
        "            if greedy:\n",
        "                action = q_values.argmax().item()\n",
        "            else:\n",
        "                action = agent.sample_actions(q_values)[0]\n",
        "\n",
        "            next_state, reward, done, i = env.step(action)\n",
        "            total_rewards += reward\n",
        "            state = next_state\n",
        "            t += 1\n",
        "\n",
        "    return total_rewards / n_games"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8v9dwQuTH4wy"
      },
      "outputs": [],
      "source": [
        "# Now we create a class ReplayBuffer. The object of this class is responsible for storing the buffer information based on the agent's action when we play the agent(i.e, current_State -> action -> next_state -> done_flag ->reward)\n",
        "# For Deep Q Learning we sample information of size = 'batch_size' from the ReplayBuffer and return that information for training\n",
        "# This buffer has a fixed size, set that to 10**6. remove previous information as new information is passed in the buffer\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "\tdef __init__(self, size):\n",
        "\t\t#TODO\n",
        "\t\t# size is the maximum size that the buffer can hold\n",
        "\t\tself.size=size\n",
        "\t\tself.buffer=[]\n",
        "\t\tself.position=0\n",
        "\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\t# no need to change\n",
        "\t\treturn len(self.buffer)\n",
        "\n",
        "\tdef add(self, state, action ,reward, next_state, done):\n",
        "\t\texperience=(state, action ,reward, next_state, done)\n",
        "\t\tif len(self.buffer)<self.size:\n",
        "\t\t\tself.buffer.append(experience)\n",
        "\t\telse:\n",
        "\t\t\tself.buffer[self.position] = experience\n",
        "\n",
        "\t\tself.position = (self.position + 1) % self.size\n",
        "\t\t#TODO\n",
        "\t\t# store the information passed in one call to add as 1 unit of informmation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\tdef sample(self, batch_size):\n",
        "\t\t#TODO\n",
        "\t\t# return a random sampling of 'batch_size' units of information\n",
        "\t\tbatch=random.sample(self.buffer,min(batch_size,len(self.buffer)))\n",
        "\t\treturn batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xW0B7tc3rOAr"
      },
      "outputs": [],
      "source": [
        "def play_and_record(start_state, agent, env, exp_replay, n_steps = 1):\n",
        "\tstate = start_state\n",
        "\tfor _ in range(n_steps):\n",
        "\t\tstate_t = torch.tensor([state], dtype=torch.float32, device=device)\n",
        "\t\tqvalues_t = agent(state_t)\n",
        "\t\tqvalues = qvalues_t.cpu().detach().numpy()[0]\n",
        "\t\taction = agent.sample_actions(qvalues)[0]\n",
        "\t\tnext_state, reward, done, _ = env.step(action)\n",
        "\t\texp_replay.add(state, action, reward, next_state, done)\n",
        "\t\tif done:\n",
        "\t\t\tstate = env.reset()\n",
        "\t\telse:\n",
        "\t\t\tstate = next_state\n",
        "\n",
        "\n",
        "\n",
        "\t# use this function to make the agent play on the env and store the information in exp_replay which is an object of class ReplayBuffer\n",
        "\t# n_steps is the number of steps to be played in this function on one call\n",
        "\t#TODO\n",
        "\t# pass\n",
        "\n",
        "\n",
        "def compute_td_loss(agent, target_network, device, batch_size, exp_replay ,gamma = 0.99,):\n",
        "\tstates, actions, rewards, next_states, dones = zip(*exp_replay.sample(batch_size))\n",
        "\tstates = torch.tensor(states,dtype=torch.float32).to(device)\n",
        "\tactions = torch.tensor(actions,dtype=torch.long).to(device)\n",
        "\trewards = torch.tensor(rewards,dtype=torch.float32).to(device)\n",
        "\tnext_states = torch.tensor(next_states,dtype=torch.float32).to(device)\n",
        "\tdones = torch.tensor(dones,dtype=torch.float32).to(device)\n",
        "\n",
        "    # Compute the predicted Q-values of the actions using the agent\n",
        "\tq_values = agent(states)\n",
        "\tpredicted_qvalues = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\tgamma=torch.tensor(gamma,dtype=torch.float32)\n",
        "\n",
        "    # Compute the target Q-values of the actions using the target network\n",
        "\twith torch.no_grad():\n",
        "\t\ttarget_q_values = target_network(next_states)\n",
        "\t\ttarget_qvalues_of_actions = rewards + torch.mul(gamma,target_q_values.max(dim=1)[0]) * torch.logical_not(dones)\n",
        "\n",
        "    # Compute the TD loss (Mean Squared Error)\n",
        "\tloss = torch.nn.MSELoss()(predicted_qvalues, target_qvalues_of_actions)\n",
        "\treturn loss\n",
        "\n",
        "\t# Here agent is the one playing on the game and target_network is updates using agent after some fixed steps as is done in Deep Q Learning\n",
        "\t# sample 'batch_size' units of info stored in the exp_replay\n",
        "\t# Find the predicted_qvalues_of_actions using agent and target_qvalues_of_actions using target_network, find the loss based on these Mean Squared Error of these two\n",
        "\t# IMPORTANT NOTE : check the type of objects, U need to convert the actions, rewards, etc, to toch.tensors for backward propogation using pytorch\n",
        "\t#TODO\n",
        "\t# pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE3kRwWVH-ja",
        "outputId": "5a6ad178-b782-4d06-e6b7-7ec67179d704"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x78d772380f30>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "############# MAIN LOOP ###############\n",
        "from tqdm import trange\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "\n",
        "seed = 108\n",
        "random.seed(108)\n",
        "np.random.seed(108)\n",
        "torch.manual_seed(108)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVSeBWUTIC75",
        "outputId": "d68cac28-7936-4f9b-d619-cdc583f41f89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "##  setup environment using make_env function defined above\n",
        "# find action_space and observation_space of the atari\n",
        "# Use env_name = \"BreakoutNoFrameskip-v4\"\n",
        "# Reset the environment before starting to train the agent and everytime the game ends (U will get a done flag which is a boolean representing whether the game has ended or not)\n",
        "# TODO\n",
        "env_name = \"BreakoutNoFrameskip-v4\"\n",
        "env = make_env(env_name)\n",
        "action_space = env.action_space\n",
        "observation_space = env.observation_space\n",
        "state_shape = observation_space.shape\n",
        "n_actions = action_space.n\n",
        "state = env.reset()\n",
        "done = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTiOUSicoJGl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq_FYxl4IKjD",
        "outputId": "76e97715-40ec-4ba0-d04f-3a8d5fb49d5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# create agent from DQNAgent class the online network\n",
        "# create target_network from DQNAgent class is updated after some fixed steps from agent\n",
        "# Note initialise target network values from agent\n",
        "# Create the online network (agent) and target network objects\n",
        "agent = DQNAgent(observation_space.shape, action_space.n, epsilon=0.1)\n",
        "target_network = DQNAgent(observation_space.shape, action_space.n, epsilon=0.1)\n",
        "\n",
        "# Initialize the target network with the agent's values\n",
        "target_network.load_state_dict(agent.state_dict())\n",
        "\n",
        "# TODO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6lL697yINov",
        "outputId": "973ba024-6028-4a80-e6c9-90a0122b7f2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replay Buffer : i :  0\n",
            "Replay Buffer : i :  1\n",
            "Replay Buffer : i :  2\n",
            "Replay Buffer : i :  3\n",
            "400\n"
          ]
        }
      ],
      "source": [
        "# created a ReplayBuffer object and saved some information in the object by playing the agent. It is better to populate some information in the Buffer, hence this step\n",
        "#filling experience replay with some samples using full random policy\n",
        "exp_replay = ReplayBuffer(10**6)\n",
        "for i in range(4):\n",
        "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)\n",
        "    print( \"Replay Buffer : i : \", i)\n",
        "    if len(exp_replay) == 10**6:\n",
        "        break\n",
        "print(len(exp_replay))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "STFEuu7IIUhL"
      },
      "outputs": [],
      "source": [
        "#setup some parameters for training\n",
        "timesteps_per_epoch = 2\n",
        "batch_size = 32\n",
        "\n",
        "total_steps = 2 * 10**6\n",
        "\n",
        "#Optimizer\n",
        "optimizer = torch.optim.Adam(agent.parameters(), lr=2e-5)\n",
        "# TODO - use Adam optimiser from torch with learning rate (lr) = 2*1e-5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "UjxwxQ6vIVKO"
      },
      "outputs": [],
      "source": [
        "#setting exploration epsilon\n",
        "start_epsilon = 0.1\n",
        "end_epsilon = 0.05\n",
        "eps_decay_final_step = 1 * 10**5\n",
        "\n",
        "# setup spme frequency for logginf and updating target network\n",
        "loss_freq = 20\n",
        "refresh_target_network_freq = 100\n",
        "eval_freq = 10000\n",
        "\n",
        "# to clip the gradients\n",
        "max_grad_norm = 5000\n",
        "\n",
        "mean_rw_history = []\n",
        "td_loss_history = []\n",
        "\n",
        "SAVE_INTERVAL = 50000\n",
        "\n",
        "from numpy import asarray\n",
        "from numpy import savetxt\n",
        "\n",
        "\n",
        "def epsilon_schedule(start_eps, end_eps, step, final_step):\n",
        "    return start_eps + (end_eps-start_eps)*min(step, final_step)/final_step\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRBO-tMJIYpq",
        "outputId": "43ecce6e-0707-4f8f-ae26-542d6ed33644"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2000001 [00:00<?, ?it/s]<ipython-input-14-88a469e385d5>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  states = torch.tensor(states, device=device, dtype=torch.float32)\n"
          ]
        }
      ],
      "source": [
        "# TODO - reset the state of the environment before starting\n",
        "state=env.reset()\n",
        "## MAIN LOOP STARTING\n",
        "for step in trange(total_steps + 1):\n",
        "\n",
        "\t#TODO update the exploration probability (epsilon) as time passes\n",
        "\t\tepsilon = epsilon_schedule(start_epsilon, end_epsilon, step, eps_decay_final_step)\n",
        "\t\tagent.epsilon = epsilon\n",
        "\t#TODO taking timesteps_per_epoch and update experience replay buffer, (use play_and_record)\n",
        "\t\tplay_and_record(state, agent, env, exp_replay, n_steps=timesteps_per_epoch)\n",
        "\t#TODO compute loss\n",
        "\t\tloss = compute_td_loss(agent, target_network, device=device, batch_size=batch_size, exp_replay=exp_replay,gamma=0.99)\n",
        "\t#TODO Backward propogation and updating the network parameters\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tloss.backward()\n",
        "\t\ttorch.nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)  # Clip gradients to avoid exploding gradients\n",
        "\t\toptimizer.step()\n",
        "\t# IMPORTANT NOTE : You only need to update the parameters of agent and not of target_network, that will be done according to refresh_target_network_freq. But Backward Propogation will take into account the target_network parameters as well. So use detach() method on target_network while calculating the loss. Google what it does and how to use !!\n",
        "\n",
        "\n",
        "\t\tif step % loss_freq == 0:\n",
        "\t\t\ttd_loss_history.append(loss.data.cpu().item())\n",
        "\n",
        "\n",
        "\t\tif step % refresh_target_network_freq == 0:\n",
        "        #TODO Load agent weights into target_network\n",
        "\t\t\ttarget_network.load_state_dict(agent.state_dict())\n",
        "\n",
        "\t\tif step % eval_freq == 0:\n",
        "\t\t\tmean_reward = evaluate(make_env(env_name, seed=step), agent, n_games=3, greedy=True, t_max=6000)\n",
        "\t\t\tmean_rw_history.append(mean_reward)\n",
        "\n",
        "\t\tprint(\"mean_reward : \", mean_reward)\n",
        "\n",
        "\t\tclear_output(True)\n",
        "\t\tprint(\"buffer size = %i, epsilon = %.5f\" %\n",
        "\t\t\t\t(len(exp_replay), agent.epsilon))\n",
        "\n",
        "\n",
        "\t\tif step % SAVE_INTERVAL == 0 and step!= 0:\n",
        "\t\t\tprint('Saving...')\n",
        "\t\t\tdevice = torch.device('cpu')\n",
        "\t\t\ttorch.save(agent.state_dict(), f'model_{step}.pth')\n",
        "\t\t\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\t\tsavetxt(f'reward_{step}.csv', np.array(mean_rw_history))\n",
        "\n",
        "\n",
        "\n",
        "# savetxt('reward_final.csv', np.array(mean_rw_history))\n",
        "\n",
        "final_score = evaluate(\n",
        "  make_env(env_name),\n",
        "  agent, n_games=1, greedy=True, t_max=10000\n",
        ")\n",
        "print('final score:', final_score)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}